#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GroZi-120 OWOD è¨“ç·´æ—¥èªŒåˆ†æå·¥å…·
åŠŸèƒ½ï¼š
1. è®€å–è¨“ç·´æ—¥èªŒä¸¦æå– loss å’ŒæŒ‡æ¨™
2. ç¹ªè£½è¨“ç·´éç¨‹ä¸­çš„ loss è®ŠåŒ–æ›²ç·š
3. ç¹ªè£½é©—è­‰æŒ‡æ¨™è®ŠåŒ–æ›²ç·š
4. ç”Ÿæˆè©³ç´°çš„è¨“ç·´å ±å‘Š
"""

import re
import json
from pathlib import Path
from datetime import datetime
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
from typing import Dict, List, Tuple
import argparse

# è¨­å®šä¸­æ–‡å­—é«”
matplotlib.rcParams['font.sans-serif'] = ['Microsoft YaHei', 'SimHei', 'SimSun', 'DejaVu Sans']
matplotlib.rcParams['axes.unicode_minus'] = False
import warnings
warnings.filterwarnings('ignore', category=UserWarning)


class GroZi120LogAnalyzer:
    def __init__(self, work_dir: str = 'work_dirs/grozi120_task1'):
        self.work_dir = Path(work_dir)
        self.train_metrics = []
        self.val_metrics = {}
        
    def find_latest_log(self) -> Path:
        """æ‰¾åˆ°æœ€æ–°çš„è¨“ç·´æ—¥èªŒ"""
        log_files = list(self.work_dir.glob('*/*.log'))
        if not log_files:
            log_files = list(self.work_dir.glob('*.log'))
        
        if not log_files:
            raise FileNotFoundError(f"åœ¨ {self.work_dir} ä¸­æ‰¾ä¸åˆ°æ—¥èªŒæ–‡ä»¶")
        
        # è¿”å›æœ€æ–°çš„æ—¥èªŒæ–‡ä»¶
        return max(log_files, key=lambda p: p.stat().st_mtime)
    
    def parse_training_log(self, log_path: Path):
        """è§£æè¨“ç·´æ—¥èªŒä¸­çš„ loss å’ŒæŒ‡æ¨™"""
        print(f"ğŸ“– è®€å–æ—¥èªŒ: {log_path}")
        
        with open(log_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        # è§£æè¨“ç·´æŒ‡æ¨™
        train_pattern = re.compile(
            r'Epoch\(train\)\s+\[(\d+)\]\[(\d+)/(\d+)\].*?'
            r'loss:\s+([\d.]+).*?'
            r'one2many_loss_cls:\s+([\d.]+).*?'
            r'one2many_loss_bbox:\s+([\d.]+).*?'
            r'one2many_loss_dfl:\s+([\d.]+).*?'
            r'one2one_loss_cls:\s+([\d.]+).*?'
            r'one2one_loss_bbox:\s+([\d.]+).*?'
            r'one2one_loss_dfl:\s+([\d.]+)'
        )
        
        for line in lines:
            match = train_pattern.search(line)
            if match:
                epoch = int(match.group(1))
                iter_cur = int(match.group(2))
                iter_total = int(match.group(3))
                
                metrics = {
                    'epoch': epoch,
                    'iter': iter_cur,
                    'iter_total': iter_total,
                    'total_loss': float(match.group(4)),
                    'one2many_loss_cls': float(match.group(5)),
                    'one2many_loss_bbox': float(match.group(6)),
                    'one2many_loss_dfl': float(match.group(7)),
                    'one2one_loss_cls': float(match.group(8)),
                    'one2one_loss_bbox': float(match.group(9)),
                    'one2one_loss_dfl': float(match.group(10)),
                }
                self.train_metrics.append(metrics)
        
        print(f"  âœ“ æ‰¾åˆ° {len(self.train_metrics)} å€‹è¨“ç·´è¿­ä»£è¨˜éŒ„")
        
        # è§£æé©—è­‰æŒ‡æ¨™
        self._parse_validation_metrics(lines)
    
    def _parse_validation_metrics(self, lines: List[str]):
        """è§£æé©—è­‰æŒ‡æ¨™"""
        current_epoch = None
        in_val_section = False
        
        for i, line in enumerate(lines):
            # æª¢æ¸¬é©—è­‰é–‹å§‹
            if 'Saving checkpoint at' in line:
                match = re.search(r'Saving checkpoint at (\d+) epochs', line)
                if match:
                    current_epoch = int(match.group(1))
                    in_val_section = True
                    self.val_metrics[current_epoch] = {}
            
            # æå–é©—è­‰æŒ‡æ¨™
            if in_val_section and current_epoch:
                # Known é¡åˆ¥æŒ‡æ¨™
                if 'Known AP50:' in line:
                    match = re.search(r'Known AP50:\s+([\d.]+)', line)
                    if match:
                        self.val_metrics[current_epoch]['known_ap50'] = float(match.group(1))
                
                if 'Known Recall50:' in line:
                    match = re.search(r'Known Recall50:\s+([\d.]+)', line)
                    if match:
                        self.val_metrics[current_epoch]['known_recall'] = float(match.group(1))
                
                if 'Known Precisions50:' in line:
                    match = re.search(r'Known Precisions50:\s+([\d.]+)', line)
                    if match:
                        self.val_metrics[current_epoch]['known_precision'] = float(match.group(1))
                
                # Unknown é¡åˆ¥æŒ‡æ¨™
                if 'Unknown AP50:' in line:
                    match = re.search(r'Unknown AP50:\s+([\d.]+)', line)
                    if match:
                        self.val_metrics[current_epoch]['unknown_ap50'] = float(match.group(1))
                
                if 'Unknown Recall50:' in line:
                    match = re.search(r'Unknown Recall50:\s+([\d.]+)', line)
                    if match:
                        self.val_metrics[current_epoch]['unknown_recall'] = float(match.group(1))
                
                if 'Wilderness Impact:' in line:
                    match = re.search(r'Wilderness Impact:\s+\{50:\s+([\d.]+)\}', line)
                    if match:
                        self.val_metrics[current_epoch]['wilderness_impact'] = float(match.group(1))
                        in_val_section = False  # é©—è­‰sectionçµæŸ
        
        print(f"  âœ“ æ‰¾åˆ° {len(self.val_metrics)} å€‹é©—è­‰é»")
    
    def plot_training_loss(self, output_dir: Path):
        """ç¹ªè£½è¨“ç·´ loss æ›²ç·š"""
        if not self.train_metrics:
            print("  âš ï¸  æ²’æœ‰è¨“ç·´æ•¸æ“šå¯ç¹ªè£½")
            return
        
        # æº–å‚™æ•¸æ“š - è¨ˆç®—æ¯å€‹ epoch çš„å¹³å‡å€¼
        epochs_data = {}
        for metric in self.train_metrics:
            epoch = metric['epoch']
            if epoch not in epochs_data:
                epochs_data[epoch] = {
                    'total_loss': [],
                    'one2many_loss_cls': [],
                    'one2many_loss_bbox': [],
                    'one2many_loss_dfl': [],
                    'one2one_loss_cls': [],
                    'one2one_loss_bbox': [],
                    'one2one_loss_dfl': [],
                }
            
            epochs_data[epoch]['total_loss'].append(metric['total_loss'])
            epochs_data[epoch]['one2many_loss_cls'].append(metric['one2many_loss_cls'])
            epochs_data[epoch]['one2many_loss_bbox'].append(metric['one2many_loss_bbox'])
            epochs_data[epoch]['one2many_loss_dfl'].append(metric['one2many_loss_dfl'])
            epochs_data[epoch]['one2one_loss_cls'].append(metric['one2one_loss_cls'])
            epochs_data[epoch]['one2one_loss_bbox'].append(metric['one2one_loss_bbox'])
            epochs_data[epoch]['one2one_loss_dfl'].append(metric['one2one_loss_dfl'])
        
        # è¨ˆç®—æ¯å€‹ epoch çš„å¹³å‡å€¼
        epochs = sorted(epochs_data.keys())
        avg_total_loss = [np.mean(epochs_data[e]['total_loss']) for e in epochs]
        avg_cls_loss = [np.mean(epochs_data[e]['one2many_loss_cls']) + 
                       np.mean(epochs_data[e]['one2one_loss_cls']) for e in epochs]
        avg_bbox_loss = [np.mean(epochs_data[e]['one2many_loss_bbox']) + 
                        np.mean(epochs_data[e]['one2one_loss_bbox']) for e in epochs]
        avg_dfl_loss = [np.mean(epochs_data[e]['one2many_loss_dfl']) + 
                       np.mean(epochs_data[e]['one2one_loss_dfl']) for e in epochs]
        
        # ç¹ªè£½ loss æ›²ç·š
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('GroZi-120 OWOD Training Loss', fontsize=16, fontweight='bold')
        
        # ç¸½ Loss
        ax = axes[0, 0]
        ax.plot(epochs, avg_total_loss, 'b-', linewidth=2, marker='o', markersize=4)
        ax.set_xlabel('Epoch', fontsize=12)
        ax.set_ylabel('Loss', fontsize=12)
        ax.set_title('Total Loss', fontsize=13, fontweight='bold')
        ax.grid(True, alpha=0.3)
        
        # åˆ†é¡ Loss
        ax = axes[0, 1]
        ax.plot(epochs, avg_cls_loss, 'r-', linewidth=2, marker='s', markersize=4)
        ax.set_xlabel('Epoch', fontsize=12)
        ax.set_ylabel('Loss', fontsize=12)
        ax.set_title('Classification Loss (One2Many + One2One)', fontsize=13, fontweight='bold')
        ax.grid(True, alpha=0.3)
        
        # BBox Loss
        ax = axes[1, 0]
        ax.plot(epochs, avg_bbox_loss, 'g-', linewidth=2, marker='^', markersize=4)
        ax.set_xlabel('Epoch', fontsize=12)
        ax.set_ylabel('Loss', fontsize=12)
        ax.set_title('BBox Loss (One2Many + One2One)', fontsize=13, fontweight='bold')
        ax.grid(True, alpha=0.3)
        
        # DFL Loss
        ax = axes[1, 1]
        ax.plot(epochs, avg_dfl_loss, 'm-', linewidth=2, marker='d', markersize=4)
        ax.set_xlabel('Epoch', fontsize=12)
        ax.set_ylabel('Loss', fontsize=12)
        ax.set_title('DFL Loss (One2Many + One2One)', fontsize=13, fontweight='bold')
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ä¿å­˜åœ–è¡¨
        output_file = output_dir / 'training_loss_curves.png'
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        print(f"  âœ“ Loss æ›²ç·šå·²ä¿å­˜: {output_file}")
        plt.close()
    
    def plot_validation_metrics(self, output_dir: Path):
        """ç¹ªè£½é©—è­‰æŒ‡æ¨™æ›²ç·š"""
        if not self.val_metrics:
            print("  âš ï¸  æ²’æœ‰é©—è­‰æ•¸æ“šå¯ç¹ªè£½")
            return
        
        epochs = sorted(self.val_metrics.keys())
        
        # æº–å‚™æ•¸æ“š
        known_ap50 = [self.val_metrics[e].get('known_ap50', np.nan) for e in epochs]
        known_recall = [self.val_metrics[e].get('known_recall', np.nan) for e in epochs]
        known_precision = [self.val_metrics[e].get('known_precision', np.nan) for e in epochs]
        unknown_ap50 = [self.val_metrics[e].get('unknown_ap50', np.nan) for e in epochs]
        unknown_recall = [self.val_metrics[e].get('unknown_recall', np.nan) for e in epochs]
        wilderness = [self.val_metrics[e].get('wilderness_impact', np.nan) for e in epochs]
        
        # ç¹ªè£½é©—è­‰æŒ‡æ¨™
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('GroZi-120 OWOD Validation Metrics', fontsize=16, fontweight='bold')
        
        # Known vs Unknown AP50
        ax = axes[0, 0]
        ax.plot(epochs, known_ap50, 'b-o', linewidth=2, markersize=6, label='Known AP50')
        ax.plot(epochs, unknown_ap50, 'r-s', linewidth=2, markersize=6, label='Unknown AP50')
        ax.set_xlabel('Epoch', fontsize=12)
        ax.set_ylabel('AP50', fontsize=12)
        ax.set_title('AP50 (Known vs Unknown)', fontsize=13, fontweight='bold')
        ax.legend(fontsize=11)
        ax.grid(True, alpha=0.3)
        
        # Known Metrics
        ax = axes[0, 1]
        ax.plot(epochs, known_ap50, 'b-o', linewidth=2, markersize=5, label='AP50')
        ax.plot(epochs, known_recall, 'g-^', linewidth=2, markersize=5, label='Recall')
        ax.plot(epochs, known_precision, 'm-s', linewidth=2, markersize=5, label='Precision')
        ax.set_xlabel('Epoch', fontsize=12)
        ax.set_ylabel('Score', fontsize=12)
        ax.set_title('Known Classes Metrics', fontsize=13, fontweight='bold')
        ax.legend(fontsize=11)
        ax.grid(True, alpha=0.3)
        
        # Unknown Recall
        ax = axes[1, 0]
        ax.plot(epochs, unknown_recall, 'r-d', linewidth=2, markersize=6)
        ax.set_xlabel('Epoch', fontsize=12)
        ax.set_ylabel('Recall (%)', fontsize=12)
        ax.set_title('Unknown Recall', fontsize=13, fontweight='bold')
        ax.grid(True, alpha=0.3)
        
        # Wilderness Impact
        ax = axes[1, 1]
        ax.plot(epochs, wilderness, 'orange', linewidth=2, marker='D', markersize=6)
        ax.set_xlabel('Epoch', fontsize=12)
        ax.set_ylabel('Wilderness Impact (è¶Šä½è¶Šå¥½)', fontsize=12)
        ax.set_title('Wilderness Impact', fontsize=13, fontweight='bold')
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ä¿å­˜åœ–è¡¨
        output_file = output_dir / 'validation_metrics_curves.png'
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        print(f"  âœ“ é©—è­‰æŒ‡æ¨™æ›²ç·šå·²ä¿å­˜: {output_file}")
        plt.close()
    
    def generate_summary_report(self, output_dir: Path, log_path: Path):
        """ç”Ÿæˆè¨“ç·´æ‘˜è¦å ±å‘Š"""
        report = {
            'log_file': str(log_path),
            'analysis_time': datetime.now().isoformat(),
            'training_summary': {
                'total_iterations': len(self.train_metrics),
                'epochs_trained': max([m['epoch'] for m in self.train_metrics]) if self.train_metrics else 0,
            },
            'validation_summary': {
                'total_validations': len(self.val_metrics),
            }
        }
        
        # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
        if self.val_metrics:
            best_epoch = max(self.val_metrics.keys(), 
                           key=lambda e: self.val_metrics[e].get('known_ap50', 0))
            report['best_model'] = {
                'epoch': best_epoch,
                'metrics': self.val_metrics[best_epoch]
            }
        
        # æœ€å¾Œä¸€å€‹ epoch çš„æ•¸æ“š
        if self.train_metrics:
            last_metrics = self.train_metrics[-1]
            report['latest_training'] = {
                'epoch': last_metrics['epoch'],
                'total_loss': last_metrics['total_loss'],
            }
        
        # ä¿å­˜ JSON
        json_file = output_dir / 'training_report.json'
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        print(f"  âœ“ JSON å ±å‘Šå·²ä¿å­˜: {json_file}")
        
        # ä¿å­˜æ–‡æœ¬å ±å‘Š
        txt_file = output_dir / 'training_report.txt'
        with open(txt_file, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("GroZi-120 OWOD Training Report\n")
            f.write("=" * 80 + "\n\n")
            f.write(f"Log File: {log_path}\n")
            f.write(f"Analysis Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("Training Summary:\n")
            f.write(f"  Total Iterations: {report['training_summary']['total_iterations']}\n")
            f.write(f"  Epochs Trained: {report['training_summary']['epochs_trained']}\n\n")
            
            if 'best_model' in report:
                f.write("Best Model:\n")
                f.write(f"  Epoch: {report['best_model']['epoch']}\n")
                for key, value in report['best_model']['metrics'].items():
                    f.write(f"  {key}: {value:.4f}\n")
        
        print(f"  âœ“ æ–‡æœ¬å ±å‘Šå·²ä¿å­˜: {txt_file}")
    
    def run(self):
        """åŸ·è¡Œå®Œæ•´åˆ†æ"""
        print("\n" + "=" * 80)
        print("GroZi-120 OWOD è¨“ç·´æ—¥èªŒåˆ†æå·¥å…·")
        print("=" * 80 + "\n")
        
        # 1. æ‰¾åˆ°æ—¥èªŒæ–‡ä»¶
        try:
            log_path = self.find_latest_log()
        except FileNotFoundError as e:
            print(f"âŒ {e}")
            return
        
        # 2. è§£ææ—¥èªŒ
        print("æ­¥é©Ÿ 1/4: è§£æè¨“ç·´æ—¥èªŒ...")
        self.parse_training_log(log_path)
        
        # 3. å‰µå»ºè¼¸å‡ºç›®éŒ„
        output_dir = self.work_dir / 'analysis'
        output_dir.mkdir(exist_ok=True)
        
        # 4. ç¹ªè£½åœ–è¡¨
        print("\næ­¥é©Ÿ 2/4: ç¹ªè£½è¨“ç·´ Loss æ›²ç·š...")
        self.plot_training_loss(output_dir)
        
        print("\næ­¥é©Ÿ 3/4: ç¹ªè£½é©—è­‰æŒ‡æ¨™æ›²ç·š...")
        self.plot_validation_metrics(output_dir)
        
        # 5. ç”Ÿæˆå ±å‘Š
        print("\næ­¥é©Ÿ 4/4: ç”Ÿæˆè¨“ç·´å ±å‘Š...")
        self.generate_summary_report(output_dir, log_path)
        
        print("\n" + "=" * 80)
        print(f"âœ… åˆ†æå®Œæˆï¼çµæœä¿å­˜åœ¨: {output_dir}")
        print("=" * 80 + "\n")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='GroZi-120 OWOD è¨“ç·´æ—¥èªŒåˆ†æ')
    parser.add_argument('--work-dir', type=str, default='work_dirs/grozi120_task1',
                       help='è¨“ç·´å·¥ä½œç›®éŒ„è·¯å¾‘')
    
    args = parser.parse_args()
    
    analyzer = GroZi120LogAnalyzer(work_dir=args.work_dir)
    analyzer.run()
