"""
GroceryDataset COCO â†’ OWOD æ ¼å¼è½‰æ›è…³æœ¬
å°‡ COCO JSON è½‰æ›ç‚º OWOD æ‰€éœ€çš„ VOC XML æ ¼å¼ï¼Œä¸¦å‰µå»º OWOD ä»»å‹™åˆ†å‰²
"""
import xml.etree.cElementTree as ET
import os
import json
import shutil
from pathlib import Path
from tqdm import tqdm
import random
import numpy as np


def create_voc_xml(image_info, annotations, categories):
    """å‰µå»º VOC æ ¼å¼ XML"""
    annotation_el = ET.Element('annotation')
    ET.SubElement(annotation_el, 'filename').text = image_info['file_name']
    
    size_el = ET.SubElement(annotation_el, 'size')
    ET.SubElement(size_el, 'width').text = str(image_info['width'])
    ET.SubElement(size_el, 'height').text = str(image_info['height'])
    ET.SubElement(size_el, 'depth').text = str(3)
    
    for ann in annotations:
        object_el = ET.SubElement(annotation_el, 'object')
        category_name = categories[ann['category_id']]
        ET.SubElement(object_el, 'name').text = category_name
        ET.SubElement(object_el, 'difficult').text = '0'
        
        bbox = ann['bbox']
        bb_el = ET.SubElement(object_el, 'bndbox')
        ET.SubElement(bb_el, 'xmin').text = str(int(bbox[0] + 1.0))
        ET.SubElement(bb_el, 'ymin').text = str(int(bbox[1] + 1.0))
        ET.SubElement(bb_el, 'xmax').text = str(int(bbox[0] + bbox[2] + 1.0))
        ET.SubElement(bb_el, 'ymax').text = str(int(bbox[1] + bbox[3] + 1.0))
    
    return ET.ElementTree(annotation_el)


def split_owod_tasks(categories, annotations, task_split=[3, 3, 3, 2]):
    """
    å°‡é¡åˆ¥åˆ†é…åˆ° OWOD ä»»å‹™ä¸­
    
    Args:
        categories: COCO æ ¼å¼çš„é¡åˆ¥åˆ—è¡¨ (å¦‚ [{'id': 0, 'name': 'category_0'}, ...])
        annotations: æ¨™è¨»åˆ—è¡¨
        task_split: æ¯å€‹ä»»å‹™æ–°å¢çš„é¡åˆ¥æ•¸ [T1, T2, T3, T4]
    
    Returns:
        task_categories: {task_id: [category_names]}
        task_images: {task_id: [image_ids]} - åŒ…å«è©²ä»»å‹™é¡åˆ¥çš„åœ–ç‰‡
    """
    # å»ºç«‹é¡åˆ¥ id â†’ name æ˜ å°„
    id_to_name = {cat['id']: cat['name'] for cat in categories}
    
    # çµ±è¨ˆæ¯å€‹é¡åˆ¥çš„æ¨£æœ¬æ•¸
    category_counts = {}
    category_images = {}
    
    for cat_name in id_to_name.values():
        category_counts[cat_name] = 0
        category_images[cat_name] = set()
    
    for ann in annotations:
        cat_name = id_to_name[ann['category_id']]
        category_counts[cat_name] += 1
        category_images[cat_name].add(ann['image_id'])
    
    # æŒ‰æ¨£æœ¬æ•¸æ’åºé¡åˆ¥ï¼ˆå¯é¸ï¼šå¹³è¡¡ä»»å‹™é›£åº¦ï¼‰
    sorted_categories = sorted(
        [(name, count) for name, count in category_counts.items()],
        key=lambda x: x[1],
        reverse=False  # False: å¾å°‘åˆ°å¤šï¼ŒTrue: å¾å¤šåˆ°å°‘
    )
    
    print("\nğŸ“Š é¡åˆ¥çµ±è¨ˆï¼ˆæŒ‰æ¨£æœ¬æ•¸æ’åºï¼‰:")
    for cat_name, count in sorted_categories:
        print(f"  {cat_name}: {count} å€‹æ¨™è¨», {len(category_images[cat_name])} å¼µåœ–ç‰‡")
    
    # åˆ†é…é¡åˆ¥åˆ°ä»»å‹™
    task_categories = {}
    task_list = [0]  # [0, 3, 6, 9, 11]
    
    current_idx = 0
    for task_id, num_classes in enumerate(task_split, 1):
        task_cats = []
        for i in range(num_classes):
            if current_idx < len(sorted_categories):
                task_cats.append(sorted_categories[current_idx][0])
                current_idx += 1
        task_categories[task_id] = task_cats
        task_list.append(task_list[-1] + len(task_cats))
        
        print(f"\nâœ… Task {task_id}: {task_cats}")
        print(f"   ç´¯è¨ˆé¡åˆ¥æ•¸: {task_list[-1]}")
    
    print(f"\nğŸ“‹ task_list = {task_list}")
    
    # ç‚ºæ¯å€‹ä»»å‹™æ”¶é›†åŒ…å«å…¶é¡åˆ¥çš„åœ–ç‰‡
    task_images = {task_id: set() for task_id in task_categories.keys()}
    for task_id, cats in task_categories.items():
        for cat_name in cats:
            task_images[task_id].update(category_images[cat_name])
    
    return task_categories, task_images, task_list


def convert_to_owod(coco_file, image_dir, output_root, 
                    dataset_name='GroceryOWOD',
                    task_split=[3, 3, 3, 2], train_ratio=0.7, seed=42):
    """
    å®Œæ•´è½‰æ›æµç¨‹
    
    Args:
        coco_file: COCO æ¨™è¨»æ–‡ä»¶è·¯å¾‘
        image_dir: åœ–ç‰‡æºç›®éŒ„
        output_root: OWOD æ•¸æ“šé›†æ ¹ç›®éŒ„ (å¦‚ data/OWOD)
        dataset_name: æ•¸æ“šé›†åç¨± (å¦‚ GroceryOWOD)
        task_split: æ¯å€‹ä»»å‹™çš„é¡åˆ¥æ•¸
        train_ratio: è¨“ç·´é›†æ¯”ä¾‹
        seed: éš¨æ©Ÿç¨®å­
    """
    random.seed(seed)
    np.random.seed(seed)
    
    print("="*60)
    print("ğŸ¯ GroceryDataset â†’ OWOD æ ¼å¼è½‰æ›")
    print("="*60)
    
    # è¼‰å…¥ COCO æ•¸æ“š
    print(f"\nğŸ“‚ è¼‰å…¥ COCO æ¨™è¨»: {coco_file}")
    with open(coco_file, 'r', encoding='utf-8') as f:
        coco_data = json.load(f)
    
    output_root = Path(output_root)
    
    # å‰µå»º OWOD æ¨™æº–ç›®éŒ„çµæ§‹
    # data/OWOD/JPEGImages/GroceryOWOD/
    # data/OWOD/Annotations/GroceryOWOD/
    # data/OWOD/ImageSets/GroceryOWOD/
    (output_root / 'JPEGImages' / dataset_name).mkdir(parents=True, exist_ok=True)
    (output_root / 'Annotations' / dataset_name).mkdir(parents=True, exist_ok=True)
    (output_root / 'ImageSets' / dataset_name).mkdir(parents=True, exist_ok=True)
    
    # é¡åˆ¥æ˜ å°„
    categories = {cat['id']: cat['name'] for cat in coco_data['categories']}
    category_list = [cat['name'] for cat in sorted(coco_data['categories'], key=lambda x: x['id'])]
    
    # åœ–ç‰‡åˆ°æ¨™è¨»çš„æ˜ å°„
    image_to_anns = {}
    for ann in coco_data['annotations']:
        image_id = ann['image_id']
        if image_id not in image_to_anns:
            image_to_anns[image_id] = []
        image_to_anns[image_id].append(ann)
    
    # OWOD ä»»å‹™åˆ†å‰²
    print(f"\nğŸ“ å‰µå»º OWOD ä»»å‹™åˆ†å‰²...")
    task_categories, task_images, task_list = split_owod_tasks(
        coco_data['categories'], coco_data['annotations'], task_split
    )
    
    # è½‰æ›ä¸¦è¤‡è£½æ•¸æ“š
    print(f"\nğŸ”„ è½‰æ› VOC XML ä¸¦è¤‡è£½åœ–ç‰‡...")
    success_count = 0
    missing_count = 0
    
    all_image_ids = []
    
    for img_info in tqdm(coco_data['images']):
        image_id = img_info['id']
        file_name = img_info['file_name']
        image_name_no_ext = Path(file_name).stem
        
        annotations = image_to_anns.get(image_id, [])
        if not annotations:
            continue
        
        # å‰µå»º XML (æ³¨æ„ï¼šOWOD ä½¿ç”¨ .jpg æ“´å±•åï¼Œä¸æ˜¯ .JPG)
        xml_tree = create_voc_xml(img_info, annotations, categories)
        xml_path = output_root / 'Annotations' / dataset_name / f'{image_name_no_ext}.xml'
        xml_tree.write(str(xml_path))
        
        # è¤‡è£½åœ–ç‰‡ (æ”¹ç‚º .jpg æ“´å±•åä»¥ç¬¦åˆ OWOD æ¨™æº–)
        possible_paths = [
            Path(image_dir) / file_name,
            Path(image_dir) / Path(file_name).name,
        ]
        
        src_image = None
        for p in possible_paths:
            if p.exists():
                src_image = p
                break
        
        if src_image:
            dst_image = output_root / 'JPEGImages' / dataset_name / f'{image_name_no_ext}.jpg'
            if not dst_image.exists():
                shutil.copy2(src_image, dst_image)
            success_count += 1
            all_image_ids.append((image_name_no_ext, image_id))
        else:
            missing_count += 1
    
    print(f"  âœ… æˆåŠŸ: {success_count} å¼µ")
    if missing_count > 0:
        print(f"  âš ï¸  ç¼ºå¤±: {missing_count} å¼µ")
    
    # å‰µå»ºè¨“ç·´/æ¸¬è©¦åˆ†å‰²
    print(f"\nğŸ“‘ å‰µå»º ImageSets...")
    random.shuffle(all_image_ids)
    n_train = int(len(all_image_ids) * train_ratio)
    
    train_image_ids = set([img_id for _, img_id in all_image_ids[:n_train]])
    test_image_ids = set([img_id for _, img_id in all_image_ids[n_train:]])
    
    print(f"  è¨“ç·´é›†: {len(train_image_ids)} å¼µ")
    print(f"  æ¸¬è©¦é›†: {len(test_image_ids)} å¼µ")
    
    # å¯«å…¥æ¸¬è©¦é›†æ–‡ä»¶
    test_file = output_root / 'ImageSets' / dataset_name / 'test.txt'
    with open(test_file, 'w') as f:
        for img_name, img_id in all_image_ids[n_train:]:
            f.write(f'{img_name}\n')
    print(f"  âœ… {test_file}")
    
    # ç‚ºæ¯å€‹ä»»å‹™å‰µå»º ImageSets
    for task_id in sorted(task_categories.keys()):
        # ç´¯è¨ˆæ‰€æœ‰å·²å­¸ç¿’çš„é¡åˆ¥
        known_categories = []
        for tid in range(1, task_id + 1):
            known_categories.extend(task_categories[tid])
        
        # t{X}_known.txt - å·²çŸ¥é¡åˆ¥åˆ—è¡¨
        known_file = output_root / 'ImageSets' / dataset_name / f't{task_id}_known.txt'
        with open(known_file, 'w') as f:
            for cat_name in known_categories:
                f.write(f'{cat_name}\n')
        
        print(f"\n  Task {task_id}:")
        print(f"    å·²çŸ¥é¡åˆ¥ ({len(known_categories)}): {known_categories}")
        print(f"    âœ… {known_file}")
        
        # t{X}_train.txt - è¨“ç·´åœ–ç‰‡åˆ—è¡¨
        # åªåŒ…å«ç•¶å‰ä»»å‹™æ–°å¢é¡åˆ¥çš„åœ–ç‰‡
        task_train_images = []
        for img_name, img_id in all_image_ids:
            if img_id not in train_image_ids:
                continue
            
            # æª¢æŸ¥åœ–ç‰‡æ˜¯å¦åŒ…å«ç•¶å‰ä»»å‹™çš„é¡åˆ¥
            img_anns = image_to_anns.get(img_id, [])
            img_categories = set([categories[ann['category_id']] for ann in img_anns])
            
            # åŒ…å«ç•¶å‰ä»»å‹™ä»»ä½•é¡åˆ¥çš„åœ–ç‰‡
            current_task_cats = set(task_categories[task_id])
            if img_categories & current_task_cats:
                task_train_images.append(img_name)
        
        train_file = output_root / 'ImageSets' / dataset_name / f't{task_id}_train.txt'
        with open(train_file, 'w') as f:
            for img_name in task_train_images:
                f.write(f'{img_name}\n')
        
        print(f"    è¨“ç·´åœ–ç‰‡: {len(task_train_images)} å¼µ")
        print(f"    âœ… {train_file}")
    
    # å‰µå»ºé¡åˆ¥æ–‡æœ¬æè¿°
    print(f"\nğŸ“ å‰µå»ºé¡åˆ¥æ–‡æœ¬æè¿°...")
    texts_dir = Path('../data/texts')
    texts_dir.mkdir(parents=True, exist_ok=True)
    texts_file = texts_dir / 'grocery_class_texts.json'
    
    class_texts = {}
    # æ ¹æ“šå¯¦éš›ç”¢å“é¡åˆ¥è‡ªå®šç¾©æè¿°
    for cat_name in category_list:
        class_texts[cat_name] = [
            cat_name,
            f"a {cat_name}",
            f"a photo of {cat_name}",
            f"{cat_name} on grocery shelf",
            f"grocery product {cat_name}",
        ]
    
    with open(texts_file, 'w', encoding='utf-8') as f:
        json.dump(class_texts, f, indent=2, ensure_ascii=False)
    print(f"  âœ… {texts_file}")
    
    # å‰µå»ºèªªæ˜æ–‡ä»¶
    readme = f"""# GroceryOWOD Dataset

## OWOD ä»»å‹™é…ç½®

```python
grocery_owod_settings = {{
    "task_list": {task_list},
    "test_image_set": "test"
}}
```

## ä»»å‹™è©³æƒ…

"""
    
    for task_id, cats in task_categories.items():
        prev_cls = task_list[task_id - 1]
        cur_cls = len(cats)
        total_cls = task_list[task_id]
        
        readme += f"""### Task {task_id}
- **æ–°å¢é¡åˆ¥**: {cur_cls} å€‹
- **ç´¯è¨ˆé¡åˆ¥**: {total_cls} å€‹
- **é¡åˆ¥åˆ—è¡¨**: {cats}

"""
    
    readme += f"""
## æ–‡ä»¶çµæ§‹ (OWOD æ¨™æº–æ ¼å¼)

```
data/OWOD/
â”œâ”€â”€ JPEGImages/{dataset_name}/       ({success_count} å¼µ .jpg åœ–ç‰‡)
â”œâ”€â”€ Annotations/{dataset_name}/      ({success_count} å€‹ .xml æ–‡ä»¶)
â””â”€â”€ ImageSets/{dataset_name}/
    â”œâ”€â”€ t1_train.txt
    â”œâ”€â”€ t1_known.txt
    â”œâ”€â”€ t2_train.txt
    â”œâ”€â”€ t2_known.txt
    â”œâ”€â”€ t3_train.txt
    â”œâ”€â”€ t3_known.txt
    â”œâ”€â”€ t4_train.txt
    â”œâ”€â”€ t4_known.txt
    â””â”€â”€ test.txt
```

## æ•¸æ“šçµ±è¨ˆ

- **ç¸½åœ–ç‰‡æ•¸**: {len(all_image_ids)}
- **è¨“ç·´é›†**: {len(train_image_ids)} å¼µ
- **æ¸¬è©¦é›†**: {len(test_image_ids)} å¼µ
- **ç¸½é¡åˆ¥æ•¸**: {len(category_list)}

## ä¸‹ä¸€æ­¥

1. ç”Ÿæˆ embeddings:
```bash
python tools/owod_scripts/extract_text_feats.py \\
    --config configs/pretrain/yolo_uniow_s_lora_bn_5e-4_100e_8gpus_obj365v1_goldg_train_lvis_minival.py \\
    --ckpt pretrained/yolo_uniow_s_lora_bn_5e-4_100e_8gpus_obj365v1_goldg_train_lvis_minival.pth \\
    --save_path embeddings/uniow-s \\
    --dataset GroceryOWOD
```

2. è¨“ç·´ Task 1:
```bash
set DATASET=GroceryOWOD
set TASK=1
python tools/train_owod.py configs/grocery_owod_ft/yolo_uniow_s_grocery_owod.py --amp
```
"""
    
    readme_file = output_root / 'ImageSets' / dataset_name / 'README.md'
    with open(readme_file, 'w', encoding='utf-8') as f:
        f.write(readme)
    print(f"  âœ… {readme_file}")
    
    print(f"\nâœ… è½‰æ›å®Œæˆï¼")
    print(f"ğŸ“‚ è¼¸å‡ºç›®éŒ„: {output_root}")
    print(f"ğŸ“ æ•¸æ“šé›†åç¨±: {dataset_name}")
    
    # é©—è­‰
    verify_owod_data(output_root, dataset_name)
    
    return output_root, dataset_name, task_list


def verify_owod_data(output_root, dataset_name='GroceryOWOD'):
    """é©—è­‰ OWOD æ•¸æ“šå®Œæ•´æ€§"""
    print(f"\nğŸ” é©—è­‰æ•¸æ“šå®Œæ•´æ€§...")
    
    output_root = Path(output_root)
    issues = []
    
    # æª¢æŸ¥ç›®éŒ„
    for dir_name in ['JPEGImages', 'Annotations', 'ImageSets']:
        dataset_dir = output_root / dir_name / dataset_name
        if not dataset_dir.exists():
            issues.append(f"âŒ ç¼ºå°‘ç›®éŒ„: {dir_name}/{dataset_name}")
        else:
            print(f"  âœ… {dir_name}/{dataset_name}/")
    
    # æª¢æŸ¥æ–‡ä»¶æ•¸é‡
    image_dir = output_root / 'JPEGImages' / dataset_name
    ann_dir = output_root / 'Annotations' / dataset_name
    
    n_images = len(list(image_dir.glob('*.jpg'))) if image_dir.exists() else 0
    n_xmls = len(list(ann_dir.glob('*.xml'))) if ann_dir.exists() else 0
    
    if n_images != n_xmls:
        issues.append(f"âš ï¸  åœ–ç‰‡æ•¸é‡ ({n_images}) èˆ‡æ¨™è¨»æ•¸é‡ ({n_xmls}) ä¸ç¬¦")
    else:
        print(f"  âœ… åœ–ç‰‡èˆ‡æ¨™è¨»æ•¸é‡ä¸€è‡´: {n_images}")
    
    # æª¢æŸ¥ ImageSets
    imageset_dir = output_root / 'ImageSets' / dataset_name
    required_files = []
    for task in [1, 2, 3, 4]:
        required_files.extend([f't{task}_train.txt', f't{task}_known.txt'])
    required_files.append('test.txt')
    
    for file_name in required_files:
        file_path = imageset_dir / file_name
        if file_path.exists():
            with open(file_path, 'r') as f:
                n_lines = len(f.readlines())
            print(f"  âœ… ImageSets/{dataset_name}/{file_name}: {n_lines} è¡Œ")
        else:
            issues.append(f"âŒ ç¼ºå°‘æ–‡ä»¶: ImageSets/{dataset_name}/{file_name}")
    
    if issues:
        print("\nâš ï¸  ç™¼ç¾å•é¡Œ:")
        for issue in issues:
            print(f"  {issue}")
        return False
    else:
        print("\nâœ… æ•¸æ“šå®Œæ•´æ€§æª¢æŸ¥é€šéï¼")
        return True


if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='å°‡ GroceryDataset è½‰æ›ç‚º OWOD æ ¼å¼')
    parser.add_argument('--coco-file', type=str, default='annotations_coco.json',
                       help='COCO æ¨™è¨»æ–‡ä»¶')
    parser.add_argument('--image-dir', type=str, default='GroceryDataset_part1/ShelfImages',
                       help='åœ–ç‰‡ç›®éŒ„')
    parser.add_argument('--output-dir', type=str, default='../data/OWOD',
                       help='OWOD æ•¸æ“šé›†æ ¹ç›®éŒ„ (å¦‚ data/OWOD)')
    parser.add_argument('--dataset-name', type=str, default='GroceryOWOD',
                       help='æ•¸æ“šé›†åç¨±å­ç›®éŒ„ (å¦‚ GroceryOWOD)')
    parser.add_argument('--task-split', type=int, nargs='+', default=[3, 3, 3, 2],
                       help='æ¯å€‹ä»»å‹™çš„é¡åˆ¥æ•¸ (ä¾‹å¦‚: 3 3 3 2)')
    parser.add_argument('--train-ratio', type=float, default=0.7,
                       help='è¨“ç·´é›†æ¯”ä¾‹')
    parser.add_argument('--seed', type=int, default=42,
                       help='éš¨æ©Ÿç¨®å­')
    parser.add_argument('--verify-only', action='store_true',
                       help='åƒ…é©—è­‰æ•¸æ“š')
    
    args = parser.parse_args()
    
    if args.verify_only:
        verify_owod_data(args.output_dir, args.dataset_name)
    else:
        convert_to_owod(
            coco_file=args.coco_file,
            image_dir=args.image_dir,
            output_root=args.output_dir,
            dataset_name=args.dataset_name,
            task_split=args.task_split,
            train_ratio=args.train_ratio,
            seed=args.seed
        )
        
        print(f"\n" + "="*60)
        print("ğŸ‰ è½‰æ›å®Œæˆï¼ä¸‹ä¸€æ­¥:")
        print("="*60)
        print("1. å‰µå»ºé…ç½®æ–‡ä»¶:")
        print("   - configs/datasets/grocery_owod_dataset.py")
        print("   - configs/grocery_owod_ft/yolo_uniow_s_grocery_owod.py")
        print("\n2. ç”Ÿæˆ embeddings:")
        print("   python tools/owod_scripts/extract_text_feats.py ...")
        print("\n3. é–‹å§‹è¨“ç·´:")
        print("   python tools/train_owod.py ...")
        print("\nè©³ç´°æ­¥é©Ÿè«‹åƒè€ƒ: OWOD_TRAINING_PLAN_zh-TW.md")
